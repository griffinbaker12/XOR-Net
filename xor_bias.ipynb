{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868de2d3-60c6-4f7c-8227-0c5020da8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fa5b93-4010-4f84-902a-4f79384eb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate=0.1):\n",
    "        self.inodes = 2\n",
    "        self.hnodes = 2\n",
    "        self.onodes = 1\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Weights initialization\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # Bias initialization\n",
    "        self.bias_hidden = np.random.normal(0.0, 1, (self.hnodes, 1))\n",
    "        self.bias_output = np.random.normal(0.0, 1, (self.onodes, 1))\n",
    "\n",
    "    def forward(self, inputs_list):\n",
    "        i = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # Hidden layer\n",
    "        hi = np.dot(self.wih, i) + self.bias_hidden\n",
    "        ho = self.sigmoid(hi)\n",
    "        \n",
    "        # Output layer\n",
    "        fi = np.dot(self.who, ho) + self.bias_output\n",
    "        fo = self.sigmoid(fi)\n",
    "        \n",
    "        return i, ho, fo\n",
    "\n",
    "    def train(self, inputs_list, target):\n",
    "        i, ho, fo = self.forward(inputs_list)\n",
    "        \n",
    "        targets = np.array(target, ndmin=2).T\n",
    "        \n",
    "        # Calculate errors\n",
    "        output_error = targets - fo\n",
    "        hidden_error = np.dot(self.who.T, output_error * fo * (1 - fo))\n",
    "        \n",
    "        # Update weights\n",
    "        self.who += self.lr * np.dot((output_error * fo * (1 - fo)), ho.T)\n",
    "        self.wih += self.lr * np.dot((hidden_error * ho * (1 - ho)), i.T)\n",
    "        \n",
    "        # Update biases\n",
    "        self.bias_output += self.lr * (output_error * fo * (1 - fo))\n",
    "        self.bias_hidden += self.lr * (hidden_error * ho * (1 - ho))\n",
    "        \n",
    "        # Calculate and return the loss\n",
    "        return 0.5 * np.sum(output_error**2)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff03aaf1-50dd-4eaf-ab44-85b1135f3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce0a8c0-9afd-4b44-95f2-f0fc05f566f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xor_data():\n",
    "    return (\n",
    "        ([0, 0], 0.01),\n",
    "        ([0, 1], 1),\n",
    "        ([1, 0], 1),\n",
    "        ([1, 1], 0.01)\n",
    "    )nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a54554-2e83-4dd7-bf3b-03a85e90c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000000\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for i, o in generate_xor_data():\n",
    "        nn.train(i, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147efbc2-7339-4567-bd51-77aeefc412bb",
   "metadata": {},
   "source": [
    "for ex in [[0, 0], [0, 1], [1, 0], [1,1]]:\n",
    "    print(ex)\n",
    "    _, _, output = nn.forward(ex)\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
